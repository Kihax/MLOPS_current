{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e90c6322-0851-4d26-9aa4-7a1879942322",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\"\"\"\n",
    "import json\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data\n",
    "file = open('data/answers.json', 'r')\n",
    "list_answers = json.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d10fe0da-737c-4fb2-8ca8-805ff3c04715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "def count_previous_answers_from_list(list_answer) -> int:\n",
    "    \"\"\"\n",
    "\n",
    "    :param list_answer: dictionary containing the \"created\" value of an answer and the campaign id\n",
    "    :return: the number of answers already validated in the campaign\n",
    "    \"\"\"\n",
    "    answer_to_process = list_answer.pop(0)\n",
    "    count = 0\n",
    "    for item in list_answer:\n",
    "        if item['campaign'] != answer_to_process['campaign']:\n",
    "            break\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "def extract_features(answer_doc, get_status: bool=True, training: bool=False):\n",
    "    \"\"\"\n",
    "\n",
    "    :param answer_doc: dict object of an answer\n",
    "    :param get_status: True if we want to extract the answer status in order to train a model\n",
    "    :param training: True if we are in training mode\n",
    "    :return: a processed dict object used to create the dataframe\n",
    "    \"\"\"\n",
    "    res = {}\n",
    "    job_dict = {\n",
    "        \"UPPER-MANAGEMENT\": [\"CEO\", \"COO\", \"CFO\", \"CTO\", \"AVP\", \"Assistant Vice President\", \"General Manager\",\n",
    "                             \"Country Manager\", \"Department Head\", \"Head of Innovation\", \"Executive Director\",\n",
    "                             \"Director\", \"Director of R&D\", \"Innovation Management & Business Development\",\n",
    "                             \"Technology Manager\", \"Managing Director\", \"Technical Director\", \"Engineering Manager\",\n",
    "                             \"Leader of Innovation Committee\", \"Regulatory Affairs Director\"],\n",
    "\n",
    "        \"SALES\": [\"Key Account Manager\", \"Innovations Markets Manager\", \"Tech Transfer Officer\",\n",
    "                  \"Global Process Analytics\", \"CGA\", \"Business Developer\", \"Supply Chain\"],\n",
    "\n",
    "        \"OPERATIONAL\": [\"Senior Manager\", \"Senior Scientist\", \"Supervisor\", \"Project Manager\", \"R&D Manager\",\n",
    "                        \"R&D\", \"Research Engineer\", \"Engineer\", \"Chemist\", \"IngÃ©nieur d'affaires\",\n",
    "                        \"Environmental Health & Safety\", \"EHS\", \"Proposal Writer\", \"Engineering\", \"Analyst\"],\n",
    "\n",
    "        \"ACADEMIC\": [\"Research\", \"PhD\", \"Professor\", \"Lecturer\", \"Graduate student\", \"Dean\", \"Associate Dean\"],\n",
    "\n",
    "        \"ENTREPRENEUR\": [\"Founder\", \"Co-founder\", \"Owner\", \"Co-owner\", \"President\", \"Managing Director\"],\n",
    "\n",
    "        \"CONSULTANT\": [\"Lead Consultant\", \"Senior Consultant\", \"Consultant\", \"Senior Advisor\", \"Advisor\",\n",
    "                       \"Scientific Medical Advisor\"]\n",
    "    }\n",
    "\n",
    "    res[\"time_elapsed\"] = 0\n",
    "    if \"time_elapsed\" in answer_doc:\n",
    "        res[\"time_elapsed\"] = answer_doc[\"time_elapsed\"]\n",
    "        \n",
    "\n",
    "    res[\"interest\"] = 0\n",
    "    if \"answers\" in answer_doc:\n",
    "        answer_count = 0\n",
    "        index_answer = []\n",
    "        for (question_type, answer) in answer_doc[\"answers\"].items():\n",
    "            if is_answered(answer):\n",
    "                answer_count += 1\n",
    "                index_answer.append(answer_count)\n",
    "                if re.search(r\"lead|partner\", question_type, re.I):\n",
    "                    if re.search(r\"comment\", question_type, re.I) and isinstance(answer, str) \\\n",
    "                            and (len(\"\".join(w for w in answer if w.isdigit())) > 6\n",
    "                                 or re.search(r\"[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,63}\", answer)): # Has an email?\n",
    "                        res[\"interest\"] += 2\n",
    "                    else:\n",
    "                        res[\"interest\"] += 1\n",
    "\n",
    "        if not training:  # if we are in training mode we don't want to ditch any answers\n",
    "            if index_answer == [1, 2, 3, 4]:\n",
    "                raise Error(\"Only 4 questions answered - rejecting the answer...\")\n",
    "\n",
    "        res[\"completion_rate\"] = answer_count / len(answer_doc[\"answers\"])\n",
    "    else:\n",
    "        res[\"completion_rate\"] = 0\n",
    "\n",
    "    res[\"job_title\"] = \"UNK\"\n",
    "    if \"job\" in answer_doc and isinstance(answer_doc[\"job\"], str):\n",
    "        for job_titles in job_dict:\n",
    "            for title in job_dict[job_titles]:\n",
    "                if title.lower() in answer_doc[\"job\"].lower():\n",
    "                    res[\"job_title\"] = job_titles\n",
    "                    break\n",
    "\n",
    "    \n",
    "    if get_status:\n",
    "        res[\"label\"] = \"VALIDATED\" if answer_doc[\"status\"] == \"VALIDATED_NO_MAIL\" else answer_doc[\"status\"]\n",
    "    res[\"previous_answers\"] = answer_doc[\"previous_answer\"]\n",
    "    \n",
    "    return res\n",
    "\n",
    "def is_answered(question) -> bool:\n",
    "    if question:\n",
    "        if question is None or question == \"None\":\n",
    "            return False\n",
    "        elif isinstance(question, dict):\n",
    "            if all(value is None for value in question.values()):\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "\n",
    "        elif isinstance(question, list):\n",
    "            if all(value is None for value in question):\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        else:\n",
    "            return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcf52fa4-6442-4f9a-8ba6-27c485d9b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def format_log(message):\n",
    "    now = time.time()\n",
    "    print(\"{}:\\t{}\".format(now, message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0da043c5-df39-4bd7-8357-7b70dbf89781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the mlflow experiment\n",
    "import mlflow\n",
    "import time\n",
    "\n",
    "def preprocess_data():\n",
    "    label_encoder = LabelEncoder()\n",
    "    data = []\n",
    "    label = []  # VALIDATED / REJECTED\n",
    "    res = {\"status\": \"not ok\"}\n",
    "    for index, answer in enumerate(answers):\n",
    "        previous_answer = count_previous_answers_from_list(answers[index:])\n",
    "        if \"answers\" in answer and \"time_elapsed\" in answer and 0 < answer[\"time_elapsed\"] < 5000:\n",
    "            answer[\"previous_answer\"] = previous_answer\n",
    "            label.append(answer[\"status\"])\n",
    "            data.append(extract_features(answer, training=True))\n",
    "    data = pd.DataFrame(data)\n",
    "    data[\"time_elapsed\"] = data[\"time_elapsed\"] / 5000\n",
    "    label_encoder.fit(data[\"job_title\"])\n",
    "    job_title = label_encoder.transform(data[\"job_title\"])\n",
    "    data[\"job_title\"] = job_title\n",
    "    return data\n",
    "\n",
    "answers = list_answers\n",
    "data = preprocess_data() # This prepares the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b597bde9-433b-491e-9488-8a065777b04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start the training here\n",
    "import mlflow\n",
    "from mlflow.models import ModelSignature, infer_signature\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "\n",
    "# Split the dataset\n",
    "# First training/test\n",
    "# Second training/validation\n",
    "# So we have tree data sets\n",
    "train, test = train_test_split(data, test_size=0.25, random_state=69)\n",
    "train_x = train.drop([\"label\"], axis=1).values\n",
    "train_y = train[[\"label\"]].values.ravel()\n",
    "test_x = test.drop([\"label\"], axis=1).values\n",
    "test_y = test[[\"label\"]].values.ravel()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_y)\n",
    "\n",
    "train_y_labels = label_encoder.transform(train_y)\n",
    "test_y_labels = label_encoder.transform(test_y)\n",
    "\n",
    "input_schema = Schema(\n",
    "    [\n",
    "        ColSpec(\"double\", \"time_elapsed\"),\n",
    "        ColSpec(\"integer\", \"interest\"),\n",
    "        ColSpec(\"double\", \"completion_rate\"),\n",
    "        ColSpec(\"integer\", \"job_title\"),\n",
    "        ColSpec(\"integer\", \"previous_answers\"),\n",
    "    ]\n",
    ")\n",
    "output_schema = Schema([ColSpec(\"string\")])\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bacbf8d-ef3e-4e49-aca4-9c466f5c9ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and labels\n",
    "def compute_weigts(train_y):\n",
    "    # 0=rejected 1=validated -> shape: [{\"0\": x}, {\"1\": y}]\n",
    "    class_weight = compute_class_weight(class_weight=\"balanced\", classes=np.array([\"REJECTED\", \"VALIDATED\"]), y=train_y) \n",
    "    class_distrib = Counter(train_y)\n",
    "    return (class_weight, class_distrib)\n",
    "\n",
    "def train_model_(params, train_x, train_y, test_x, test_y):\n",
    "    model = RandomForestClassifier(**params, \n",
    "        bootstrap=True, \n",
    "        max_features=\"sqrt\", \n",
    "        min_samples_leaf=1, \n",
    "        min_samples_split=2,\n",
    "        warm_start=True)\n",
    "    model.fit(train_x, train_y_labels)\n",
    "    return model\n",
    "    \n",
    "# Train\n",
    "def train_model(params, train_x, train_y, test_x, test_y):\n",
    "    with mlflow.start_run(nested=True):\n",
    "        model = train_model_(params, train_x, train_y, test_x, test_y)\n",
    "        scores_rf = model.predict_proba(test_x)[:, 1]\n",
    "        predicted_rf = [*map(lambda x: 1 if x > 0.5 else 0, scores_rf)]\n",
    "        accuracy = accuracy_score(y_true=test_y_labels, y_pred=predicted_rf)\n",
    "\n",
    "        # Log parameters and results\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"loss\", 1.0 - accuracy)\n",
    "\n",
    "        return {\"loss\": 1.0 - accuracy, \"status\": STATUS_OK, \"model\": model}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49b26153-eb18-4cc0-8bae-69665ae080fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    # MLflow will track the parameters and results for each run\n",
    "    result = train_model(\n",
    "        params,\n",
    "        train_x=train_x,\n",
    "        train_y=train_y,\n",
    "        test_x=test_x,\n",
    "        test_y=test_y,\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28363a39-a885-4633-9976-eb27777272b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "class_weight, class_distrib = compute_weigts(train_y)\n",
    "#This is for Hyperopt\n",
    "space = {\n",
    "    \"n_estimators\": hp.randint(\"n_estimators\", 50, 600),\n",
    "    \"max_depth\": hp.randint(\"max_depth\", 10, 100),\n",
    "    \"class_weight\": {0: class_weight[0], 1: class_weight[1]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27ffad5c-ceab-43bb-bf81-57085d9a1b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/04/29 15:25:07 INFO mlflow.tracking.fluent: Experiment with name '/mail-answer-prediction' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 4/4 [01:58<00:00, 29.69s/trial, best loss: 0.24651210504718912]\n",
      "Best parameters: {'max_depth': 19, 'n_estimators': 149}\n",
      "Best eval accuracy: 0.7534878949528109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'answers_classification'.\n",
      "2024/04/29 15:27:50 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: answers_classification, version 1\n",
      "Created version '1' of model 'answers_classification'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Try to configure MLFlow experiment here\n",
    "mlflow.set_tracking_uri(uri=\"http://mlflow-server:8080\")\n",
    "mlflow.set_experiment(\"/mail-answer-prediction\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Set a tag that we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"NLP training test\", \"Answers classification\")\n",
    "\n",
    "    # Conduct the hyperparameter search using Hyperopt\n",
    "    trials = Trials()\n",
    "    best = fmin(\n",
    "        fn=objective,\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=4,\n",
    "        trials=trials,\n",
    "    )\n",
    "\n",
    "    # Fetch the details of the best run\n",
    "    best_run = sorted(trials.results, key=lambda x: x[\"loss\"])[0]\n",
    "\n",
    "    # Log the best parameters, loss, and model\n",
    "    mlflow.log_params(best)\n",
    "    mlflow.log_metric(\"accuracy\", 1.0 - best_run[\"loss\"])\n",
    "    mlflow.log_metric(\"loss\", best_run[\"loss\"])\n",
    "    \n",
    "\n",
    "    # Print out the best parameters and corresponding loss\n",
    "    print(f\"Best parameters: {best}\")\n",
    "    print(f\"Best eval accuracy: {1.0 - best_run['loss']}\")\n",
    "\n",
    "    model = train_model_(best, train_x, train_y, test_x, test_y)\n",
    "    \n",
    "    # Log the model\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=model,\n",
    "        artifact_path=\"answers_classification\",\n",
    "        signature=signature,\n",
    "        input_example=train_x,\n",
    "        registered_model_name=\"answers_classification\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc06058-9d4a-428a-acb0-0654500761e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
