{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "114a21e0-3035-4c81-a7cf-39ff2bf3abff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyTDC\n",
      "  Using cached pytdc-1.1.12.tar.gz (151 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting accelerate==0.33.0 (from PyTDC)\n",
      "  Using cached accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting dataclasses<1.0,>=0.6 (from PyTDC)\n",
      "  Using cached dataclasses-0.6-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting datasets<2.20.0 (from PyTDC)\n",
      "  Using cached datasets-2.19.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting evaluate==0.4.2 (from PyTDC)\n",
      "  Using cached evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting fuzzywuzzy<1.0,>=0.18.0 (from PyTDC)\n",
      "  Using cached fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting huggingface_hub<1.0,>=0.20.3 (from PyTDC)\n",
      "  Using cached huggingface_hub-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy<2.0.0,>=1.26.4 (from PyTDC)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Collecting openpyxl<4.0.0,>=3.0.10 (from PyTDC)\n",
      "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting pandas<3.0.0,>=2.1.4 (from PyTDC)\n",
      "  Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting requests<3.0.0,>=2.31.0 (from PyTDC)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting scikit-learn==1.2.2 (from PyTDC)\n",
      "  Using cached scikit-learn-1.2.2.tar.gz (7.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [33 lines of output]\n",
      "      Partial import of sklearn during the build process.\n",
      "      Traceback (most recent call last):\n",
      "        File \"C:\\Users\\mhpie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "          main()\n",
      "        File \"C:\\Users\\mhpie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\mhpie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 149, in prepare_metadata_for_build_wheel\n",
      "          return hook(metadata_directory, config_settings)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\mhpie\\AppData\\Local\\Temp\\pip-build-env-vlmthz65\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 377, in prepare_metadata_for_build_wheel\n",
      "          self.run_setup()\n",
      "        File \"C:\\Users\\mhpie\\AppData\\Local\\Temp\\pip-build-env-vlmthz65\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 522, in run_setup\n",
      "          super().run_setup(setup_script=setup_script)\n",
      "        File \"C:\\Users\\mhpie\\AppData\\Local\\Temp\\pip-build-env-vlmthz65\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 320, in run_setup\n",
      "          exec(code, locals())\n",
      "        File \"<string>\", line 669, in <module>\n",
      "        File \"<string>\", line 663, in setup_package\n",
      "        File \"<string>\", line 597, in configure_extension_modules\n",
      "        File \"C:\\Users\\mhpie\\AppData\\Local\\Temp\\pip-install-6num9p34\\scikit-learn_121aa0f2f5b3498e97ab243da06bb96b\\sklearn\\_build_utils\\__init__.py\", line 47, in cythonize_extensions\n",
      "          basic_check_build()\n",
      "        File \"C:\\Users\\mhpie\\AppData\\Local\\Temp\\pip-install-6num9p34\\scikit-learn_121aa0f2f5b3498e97ab243da06bb96b\\sklearn\\_build_utils\\pre_build_helpers.py\", line 82, in basic_check_build\n",
      "          compile_test_program(code)\n",
      "        File \"C:\\Users\\mhpie\\AppData\\Local\\Temp\\pip-install-6num9p34\\scikit-learn_121aa0f2f5b3498e97ab243da06bb96b\\sklearn\\_build_utils\\pre_build_helpers.py\", line 38, in compile_test_program\n",
      "          ccompiler.compile(\n",
      "        File \"C:\\Users\\mhpie\\AppData\\Local\\Temp\\pip-build-env-vlmthz65\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\_msvccompiler.py\", line 379, in compile\n",
      "          self.initialize()\n",
      "        File \"C:\\Users\\mhpie\\AppData\\Local\\Temp\\pip-build-env-vlmthz65\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\_msvccompiler.py\", line 289, in initialize\n",
      "          vc_env = _get_vc_env(plat_spec)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\mhpie\\AppData\\Local\\Temp\\pip-build-env-vlmthz65\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\_msvccompiler.py\", line 150, in _get_vc_env\n",
      "          raise DistutilsPlatformError(\n",
      "      distutils.errors.DistutilsPlatformError: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!pip install PyTDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f9feb10-6b8b-451e-a105-ea8499a1dac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\mhpie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\mhpie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (2.1.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\mhpie\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\mhpie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\mhpie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "709cfe2e-8b93-479f-96bf-6d897ce0d08a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tdc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtdc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_pred\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrialOutcome\n\u001b[0;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m TrialOutcome(name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphase1\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# 'phase2' / 'phase3'\u001b[39;00m\n\u001b[0;32m      3\u001b[0m split \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget_split()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tdc'"
     ]
    }
   ],
   "source": [
    "from tdc.multi_pred import TrialOutcome\n",
    "data = TrialOutcome(name = 'phase1') # 'phase2' / 'phase3'\n",
    "split = data.get_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "976cc022-c0fc-461f-a852-9458c879d1cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msplit\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'split' is not defined"
     ]
    }
   ],
   "source": [
    "split[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08974b5c-9fbd-4c97-b9a8-95f55e9235d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def get_timestamp(date_str):\n",
    "    try:\n",
    "        # Cas où le jour est présent (ex: \"November 2, 2018\")\n",
    "        date_obj = datetime.strptime(date_str, \"%B %d, %Y\")\n",
    "    except ValueError:\n",
    "        # Cas où seul le mois et l'année sont présents (ex: \"July 2000\")\n",
    "        date_obj = datetime.strptime(date_str, \"%B %Y\")\n",
    "        date_obj = date_obj.replace(day=1)  # Ajouter le 1er du mois\n",
    "\n",
    "    return int(date_obj.timestamp())  # Convertir en timestamp (secondes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75e9822b-9c4a-4360-b8fd-99e475ba78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialisation des encodeurs\n",
    "drug_encoder = TfidfVectorizer()\n",
    "icd_encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "eligibility_encoder = TfidfVectorizer()\n",
    "duration_scaler = StandardScaler()\n",
    "\n",
    "def compute_duration(start_date, complete_date):\n",
    "    \"\"\" Calcule la durée en gérant les erreurs \"\"\"\n",
    "    duration = []\n",
    "    for i in range(len(complete_date)):\n",
    "        try:\n",
    "            duration.append(get_timestamp(complete_date[i]) - get_timestamp(start_date[i]))\n",
    "        except:\n",
    "            duration.append(10**20)  # Valeur extrême pour éviter les erreurs\n",
    "    return np.array(duration).reshape(-1, 1)  # S'assurer que c'est bien une colonne\n",
    "\n",
    "def fit_transform_data(data):\n",
    "    \"\"\" Applique fit_transform sur les données d'entraînement \"\"\"\n",
    "    global drug_encoder, icd_encoder, eligibility_encoder, duration_scaler  # On garde les mêmes encodeurs pour le test\n",
    "    \n",
    "    # Durée normalisée\n",
    "    duration = compute_duration(data[\"start_date\"], data[\"complete_date\"])\n",
    "    duration = duration_scaler.fit_transform(duration)  # Normalisation\n",
    "\n",
    "    # Encodage des features\n",
    "    drug_molecules = drug_encoder.fit_transform(np.array(data[\"drug_molecules\"])).toarray()\n",
    "    icdcodes = icd_encoder.fit_transform(np.array(data[\"icdcodes\"]).reshape(-1, 1))\n",
    "    eligibility_criteria = eligibility_encoder.fit_transform(np.array(data[\"eligibility_criteria\"])).toarray()\n",
    "\n",
    "    # Concaténation des features\n",
    "    X_train = np.concatenate([duration, drug_molecules, icdcodes, eligibility_criteria], axis=1)\n",
    "    return X_train\n",
    "\n",
    "def transform_data(data):\n",
    "    \"\"\" Applique transform sur les données de test (sans réentraîner les encodeurs) \"\"\"\n",
    "    global drug_encoder, icd_encoder, eligibility_encoder, duration_scaler\n",
    "\n",
    "    duration = compute_duration(data[\"start_date\"], data[\"complete_date\"])\n",
    "    duration = duration_scaler.transform(duration)  # Normalisation\n",
    "\n",
    "    drug_molecules = drug_encoder.transform(np.array(data[\"drug_molecules\"])).toarray()\n",
    "    icdcodes = icd_encoder.transform(np.array(data[\"icdcodes\"]).reshape(-1, 1))\n",
    "    eligibility_criteria = eligibility_encoder.transform(np.array(data[\"eligibility_criteria\"])).toarray()\n",
    "\n",
    "    X_test = np.concatenate([duration, drug_molecules, icdcodes, eligibility_criteria], axis=1)\n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e3ef580-d582-4b69-baab-afa12af510e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = fit_transform_data(split[\"train\"])  # Entraînement\n",
    "Y_train_string = split[\"train\"][\"Y\"]\n",
    "Y_train = []\n",
    "\n",
    "for k in range(len(Y_train_string)):\n",
    "    Y_train.append(int(Y_train_string[k]))\n",
    "\n",
    "Y_train = np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1be6387d-5937-4f6f-97ec-f8c5092902c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.6148034 ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.10400629,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.10400629,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [-0.10400629,  0.        ,  0.        , ...,  0.11414377,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.10400629,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.10400629,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd314b64-b844-44da-ac71-872bd29540b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = transform_data(split[\"test\"])  # Transformation des données de test sans réentraîner\n",
    "\n",
    "Y_test_string = split[\"test\"][\"Y\"]\n",
    "Y_test = []\n",
    "for k in range(len(Y_test_string)):\n",
    "    Y_test.append(int(Y_test_string[k]))\n",
    "\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903d988c-9033-4f68-a171-4d04649e67ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model random lineaire\n",
    "from sklearn.linear_model import LogisticRegression  # Or LinearRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Initialize the model (for logistic regression if Y is binary)\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "Y_pred = model.predict(X_test)  # Assuming X_test and Y_test are available for evaluation\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(Y_test, Y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f32120-41a2-40f4-9246-90ad0b397c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Paramètres à tester pour le RandomForest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialiser le modèle\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Effectuer la recherche sur les hyperparamètres avec une validation croisée\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Entraînement du modèle sur les données d'entraînement\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Meilleur modèle trouvé\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Faire des prédictions\n",
    "Y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Évaluation du modèle\n",
    "print(\"Accuracy:\", accuracy_score(Y_test, Y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(Y_test, Y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c84902e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m         mol_list\u001b[38;5;241m.\u001b[39mappend(mol_mol_list)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mol_list\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28mlen\u001b[39m(list_mol(\u001b[43msplit\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'split' is not defined"
     ]
    }
   ],
   "source": [
    "def list_mol(df):\n",
    "    dico={}\n",
    "    def molecule_existante(df):\n",
    "        for i in df:\n",
    "            j=i.split('_')\n",
    "            for mol in j:\n",
    "                if mol in dico:\n",
    "                    dico[mol]+=1\n",
    "                else:\n",
    "                    dico[mol]=1\n",
    "    molecule_existante(list(df[\"icdcodes\"]))\n",
    "    mol_list=[]\n",
    "    for mol in list(df[\"icdcodes\"]):\n",
    "        mol_mol_list=[]\n",
    "        for list_mol in dico.keys():\n",
    "            if  list_mol in mol:\n",
    "                mol_mol_list.append(1)\n",
    "            else:\n",
    "                mol_mol_list.append(0)\n",
    "        mol_list.append(mol_mol_list)\n",
    "    return mol_list\n",
    "\n",
    "len(list_mol(split[\"train\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
